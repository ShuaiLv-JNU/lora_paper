# <font style="color:rgb(28, 31, 35);">2409-DiSHA: Dimension-Sharding Adaptation of Large Language Models with Fast Convergence and Fast Computation</font>
> [https://arxiv.org/pdf/2409.15371](https://arxiv.org/pdf/2409.15371)
>

在大语言模型（LLMs）的应用中，常常需要对模型进行微调，让它能更好地完成各种下游任务。但直接对大模型进行全面微调，计算成本实在太高。所以， Parameter-Efficient Fine-Tuning（PEFT）技术应运而生，其中Low-Rank Adaptation（LoRA）是备受欢迎的一种方法。LoRA的原理是，对于一个预训练好的权重矩阵$ W_{0} \in \mathbb{R}^{d ×k} $ ，把对它的更新表示成一个低秩分解的形式$ w_{0}+\Delta W=W_{0}+BA $，这里$ B \in \mathbb{R}^{d ×r} $ ，$ A \in \mathbb{R}^{r ×k} $  ，并且秩$ r \ll min (d, k) $ 。在训练的时候，$ w_{0} $是固定不变的，不会接收梯度更新，只有A和B包含可训练的参数。这样虽然减少了可训练参数的数量，但是它有个明显的问题，就是收敛速度很慢。

为了解决LoRA收敛慢的问题，研究人员提出了不少LoRA的变体，像LoRA+通过给矩阵A和B设置不同但固定比例的学习率，来提高效率；PiSSA利用奇异值分解（SVD）优化矩阵初始化；OLoRA借助QR分解让适应矩阵正交初始化 。不过，这些方法在加快收敛速度的同时，也让初始化过程变得更加复杂，花费的时间更多。

基于这样的背景，这篇论文提出了一种新的方法。首先是Dimension-Sharding Adaptation（DiSHA）框架，它的设计思路是把预训练的权重矩阵$ W_{0} $进行划分。在训练时，$ W_{0} $同样被冻结，不接收梯度更新，更新通过一个低秩分解$ W=W_{0}+\Delta W = W_{0} + expand(D) $来实现 ，这里$ D \in \mathbb{R}^{r_{1} ×r_{2}} $  ，$ expand(D)\in \mathbb{R}^{d ×k} $  ，并且秩$ (r_{1}, r_{2}) \ll min (d, k) $ 。$ expand(D) $的具体操作是把$ w_{0} $沿着D的维度进行分片，D矩阵会根据分片数量复制，然后恢复维度和$ w_{0} $匹配，最后把更新后的分片加起来更新$ w_{0} $ 。DiSHA有两个很重要的设计空间因素：一个是秩选择，$ r_{1} $和$ r_{2} $可以独立控制参数化的复杂程度；另一个是分区策略，输出维度d可以被划分成N个大小为$ {s_{1}, ..., s_{N}} $的分片，每个分片的更新由D的一行来缩放。这样的设计可以让可训练参数变得更少，还支持非均匀分区，给模型微调提供了更灵活的方式。

但是DiSHA的设计空间太大了，对于使用者来说，很难确定合适的训练值。于是，研究人员又提出了Block Affine Efficient Computation（Bone）。Bone重新定义了可训练矩阵为$ D \in \mathbb{R}^{r ×d} $  ，只有一个秩参数$ r $ 。它的核心创新点是块输入聚合，把输入$ x \in \mathbb{R}^{b ×l ×k} $  的k维度划分成$ r $个连续的块，每个块大小是$ g=\lfloor k / r\rfloor $  ，然后对每个划分后的块沿着k维度进行求和，再通过和D矩阵相乘来计算低秩更新。从数学上来说，Bone的块求和操作和DiSHA的$ expand(D) $操作在结构稀疏的情况下是等价的。这种等价性让Bone不需要显式地计算$ expand(D) $，在内存使用上从$ O(dk) $减少到$ O(dr) $ ，计算量（FLOPs）也从$ O(bldk) $减少到$ O(blr(d+\frac{k}{r})) $ ，大大提高了计算效率。

在实验中发现，Bone虽然表现很好，在自然语言理解（NLU）和自然语言生成（NLG）任务上都超越了很多LoRA的变体，在计算效率和内存使用上也更有优势，但是它存在一个问题：相同矩阵内不同分片的更新是共线的。这就限制了模型的表达能力。为了解决这个问题，研究人员提出了Block Affine Transformation（Bat）。Bat的关键在于利用预训练权重$ w_{o} $作为非线性投影仪。它先对$ W_{0} $和$ D $进行张量分解，把$ W_{0} \in \mathbb{R}^{d ×k} $  重塑成$ W_{0} \in \mathbb{R}^{\frac{k}{r} ×\frac{d}{r} ×r×r} $  ，把$ D \in \mathbb{R}^{r ×d} $  重塑成$ D \in \mathbb{R}^{\frac{d}{r} ×r ×r} $  ；然后通过张量收缩计算分片特定的更新$ \Delta \mathcal{W}=\mathcal{W}_{0} × \mathcal{D}+\mathcal{D} \in \mathbb{R}^{\frac{k}{r} × \frac{d}{r} × r × r} $ ，这里的$ \times_{3} $表示沿着第三个维度的收缩；最后再把$ \Delta W $重塑回完整的更新矩阵。通过这样的方式，Bat引入了和$ w_{0} $的奇异向量成比例的分片相关扰动，打破了Bone中共享D导致的更新共线性，而且没有增加额外的参数，提升了模型的性能。

研究人员通过大量实验验证了这些方法的有效性。在自然语言理解实验中，对RoBERTa-base模型在GLUE基准测试的多个数据集上进行微调，Bone在CoLA数据集上收敛速度更快，数据拟合能力更强。在自然语言生成实验中，对不同的LLM模型在数学、代码、聊天等任务上进行微调，Bone在收敛速度、数据拟合和泛化能力上都优于其他PEFT方法。同时，研究人员还对比了Bone和Bat，发现Bat的非线性更新确实能提升模型性能。并且研究了不同分组方式对Bat性能的影响，发现虽然不同分组方式在某些模型上表现有差异，但总体来说，Bat都能有效地拟合数据，关键影响因素还是块大小。在资源和效率方面，对比LoRA、Bone和Bat，Bone计算效率最高，内存使用也更高效，而Bat虽然提升了模型性能，但内存使用量较大，计算速度较慢。

总的来说，DiSHA框架为大语言模型的高效自适应微调提供了新的方向，Bone在实际应用中展现出了和LoRA相当的效率优势，Bat则解决了Bone更新共线的问题，提升了模型的表达能力。未来，研究人员计划开发专门的优化算子来解决Bat计算效率低和资源消耗大的问题，也期待社区能对这些方法进行更多的测试和研究。

# <font style="color:rgb(28, 31, 35);">2303-ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING</font>
> [https://arxiv.org/pdf/2303.10512](https://arxiv.org/pdf/2303.10512)
>

在自然语言处理领域，预训练语言模型（PLMs）展现出了强大的能力，但对其进行微调时却面临着挑战。这篇论文提出的AdaLoRA方法，很好地解决了这些问题，下面为你详细解读。

1. **研究背景与问题提出**：PLMs虽然在各类自然语言处理任务中表现出色，然而其参数量巨大，像BERT可能有300百万参数，T5多达110亿参数，GPT-3更是高达1750亿参数。当用这些模型处理多个下游任务时，如果采用传统的全量微调（即对模型所有参数进行调整），每个任务都要保留一份完整的大模型副本，这会导致内存消耗极大，成本过高。

为了解决这个问题，研究者们提出了两类方法。一类是添加小的神经模块，只对这些模块进行微调，例如adapter tuning（在模型层间插入小模块）、prefix tuning（给输入或隐藏层添加可训练前缀）等；另一类是对预训练权重的增量更新进行参数高效建模，比如diff pruning（将增量更新建模为稀疏矩阵）和LoRA（通过两个小矩阵的乘积对增量进行建模）。

不过，LoRA存在明显不足。它为所有增量矩阵预设相同的秩，没有考虑到不同模块和层的权重矩阵重要性其实差异很大。举个例子，实验发现对前馈网络（FFN）模块微调的效果，要比自注意力模块好；而且顶层的权重矩阵，对模型性能的影响比底层更大。所以，如何根据模块的重要性来灵活分配参数预算，以提升微调效果，就成了亟待解决的问题。  
2. **AdaLoRA方法详解**  
    - **SVD - 基于的自适应（SVD - based adaptation）**：为了避开直接计算奇异值分解（SVD）带来的高计算成本（直接计算SVD的复杂度为$ O(min (d_{1}, d_{2}) d_{1} d_{2}) $ ，对于大量高维矩阵计算开销极大），AdaLoRA把预训练权重矩阵的增量更新用类似SVD的形式来表示：$ W = W^{(0)} + \Delta = W^{(0)} + P\Lambda Q $。这里，$ P \in \mathbb{R}^{d_{1}×r} $和$ Q \in \mathbb{R}^{r×d_{2}} $分别代表$ \Delta $的左右奇异向量矩阵，对角矩阵$ \Lambda \in \mathbb{R}^{r×r} $包含奇异值$ {\lambda_{i}}_{1 ≤i ≤r} $，并且$ r \ll min (d_{1}, d_{2}) $ 。同时，为了保证$ P $和$ Q $的正交性（即$ P^{\top}P = QQ^{\top} = I $ ），添加了正则化项$ R(P, Q) = \left\| P^{\top}P - I\right\|_{F}^{2} + \left\| QQ^{\top} - I\right\|_{F}^{2} $ 。这样一来，既避免了复杂的SVD计算，又能保证模型的稳定性。  
    - **基于重要性的秩分配（Importance - aware rank allocation）**：AdaLoRA把上述基于SVD的自适应方法应用到每个权重矩阵上。在训练过程中，通过不断修剪奇异值来控制参数预算，而修剪的依据就是重要性分数。计算重要性分数有多种方式，比如简单地根据奇异值大小（$ S_{k, i} = |\lambda_{k, i}| $ ），但这种方法不能准确衡量参数对模型性能的贡献。论文提出了一种更有效的重要性度量方式：$ S_{k, i} = s(\lambda_{k, i}) + \frac{1}{d_{1}}\sum_{j = 1}^{d_{1}}s(P_{k, ji}) + \frac{1}{d_{2}}\sum_{j = 1}^{d_{2}}s(Q_{k, ij}) $。其中，$ s(\cdot) $是针对单个参数的重要性函数，这里采用的是基于敏感性变体的计算方式：$ s^{(t)}(w_{ij})=\overline{I}^{(t)}(w_{ij}) \cdot \overline{U}^{(t)}(w_{ij}) $  ，综合考虑了奇异值和向量对模型性能的影响。计算出重要性分数后，就可以根据分数高低，给重要性高的增量矩阵分配更多预算（保留较大的奇异值），对不重要的则减少预算（将奇异值置零）。  
    - **全局预算调度器（Global budget scheduler）**：为了让训练过程更顺利，AdaLoRA设计了全局预算调度器。训练开始时，设置一个比最终目标预算稍高的初始预算$ b^{(0)} $（比如是目标预算$ b^{(T)} $的1.5倍 ），并把每个增量矩阵的初始秩设为$ r = b^{(0)} / n $（$ n $是适配权重矩阵的数量）。先进行$ t_{i} $步的热身训练，然后按照特定的三次函数调度方式逐渐降低预算$ b^{(t)} $ ，直到达到目标预算$ b^{(T)} $ 。最后，固定预算分布，再对模型进行$ t_{f} $步的微调。通过这种方式，模型可以先充分探索参数空间，之后再聚焦于最重要的权重，提升训练效果。  
3. **实验验证**：为了验证AdaLoRA的有效性，研究者在多种任务和模型上进行了大量实验。  
    - **自然语言理解任务**：在GLUE基准测试中，选用DeBERTaV3 - base模型。结果显示，无论在何种预算设置下，AdaLoRA的性能都优于或与其他方法相当。比如当参数预算为0.3M时，在RTE任务上，AdaLoRA的准确率比表现最佳的基线方法高出1.8%。  
    - **问答任务**：在SQuAD v1.1和SQuADv2.0数据集上，同样使用DeBERTaV3 - base模型进行实验。在所有预算水平下，AdaLoRA在精确匹配（EM）和F1指标上均超过现有方法。以SQuADv2.0数据集为例，当预算仅为0.08%时，AdaLoRA的F1值就达到了88.7%，不仅接近高预算下的表现，还比表现最好的基线方法高出1.2%。  
    - **自然语言生成任务**：在XSum和CNN/DailyMail数据集上，对BART - large模型进行微调实验。结果表明，在不同预算水平下，AdaLoRA在这两个数据集上的性能都优于或与基线方法相当。例如在预算为1.10%时，XSum数据集上AdaLoRA的R - 2分数为21.13，而LoRA只有19.89。  
4. **研究结论**：AdaLoRA通过创新的重要性评分机制来自适应地分配参数预算，显著提升了模型性能和参数利用效率。在自然语言处理、问答和自然语言生成等多个任务上的实验结果都有力证明，该方法比现有方法更具优势。

# <font style="color:rgb(28, 31, 35);">2312-A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA</font>
> [https://arxiv.org/pdf/2312.03732](https://arxiv.org/pdf/2312.03732)
>

在大语言模型（LLMs）的应用场景中，当我们想让模型在特定任务上表现得更好时，通常会对其进行微调。不过，直接对整个模型的大量参数进行微调，会耗费超多的计算资源和时间，就像要把一座大房子里的所有东西都重新整理一遍，既麻烦又费力。所以，人们就想出了一些更“聪明”的办法，参数高效微调（PEFT）方法就是其中一类，而低秩适配器（LoRA）是PEFT里很受欢迎的一种。这篇论文主要就研究LoRA里一个很关键的问题——缩放因子，并且提出了改进的方法，叫秩稳定的LoRA（rsLoRA）。 

1. **LoRA方法的现状与问题**：LoRA的基本想法是基于一个发现，就是预训练的LLM参数微调其实在一个低维的“空间”（专业叫低内在维度流形）里进行就够了。所以，它不在所有参数上大动干戈，而是在模型的一些层上添加小的“插件”，也就是适配器。这个适配器是由两个矩阵$ A $和$ B $相乘，再乘上一个缩放因子$ \gamma_{r} $得到的。用数学公式表示就是，原本模型里一个线性子模块是$ x_{out }=W x_{in }+b $，加上LoRA适配器后就变成了$ x_{out }=\left(W+\gamma_{r} B A\right) x_{in }+b $。这里的$ W $和$ b $是原来模型的参数 ，$ A $和$ B $是新添加适配器的参数，$ \gamma_{r} $就是缩放因子。在实际用LoRA的时候，$ \gamma_{r} $一般被设置成$ \frac{\alpha}{r} $（$ \alpha $是可以调整的超参数，$ r $代表适配器的秩，简单理解就是衡量适配器复杂程度的一个数 ）。但这么设置会出问题，随着$ r $增大，训练的时候梯度会崩溃。这就好比开车，本来想加速前进（提高性能），结果车的发动机（梯度）突然出故障，速度根本提不上去，导致高秩的LoRA适配器和低秩的比起来，性能没什么提升，大家就只能用低秩的适配器，限制了LoRA的发挥。 
2. **rsLoRA方法的思考与提出**：那为什么会这样呢？作者就想找到更好的缩放因子设置。这里作者定义了一个“秩稳定”适配器的概念，满足两个条件： 
    - 第一个条件和输入输出的统计特征有关。假设输入是一些相互独立且有相同分布的数据（iid），如果输入数据每个元素的某一阶统计特征（用$ m $阶矩衡量，简单理解就是一种描述数据分布特征的数值 ）是$ \Theta_{r}(1) $（表示和$ r $相关的一个量级 ），经过适配器处理后，输出数据每个元素的$ m $阶矩也得是$ \Theta_{r}(1) $，也就是输入输出数据的这个统计特征量级得保持稳定。 
    - 第二个条件和训练过程中的梯度有关。在训练模型时，会计算损失函数对适配器输出的梯度，如果这个梯度每个元素是$ \Theta_{r}(1) $，那么经过适配器反向传播到输入的梯度，每个元素也得是$ \Theta_{r}(1) $。这是为了保证在训练过程中，梯度在通过适配器时不会出现异常变化，让训练能稳定进行。   
为了满足这两个条件，作者通过分析当$ r $变得很大（$ r \to \infty $）时的情况，发现只有把$ \gamma_{r} $设置成$ \frac{\alpha}{\sqrt{r}} $，才能让适配器满足秩稳定的要求。基于这个新的缩放因子设置的方法，就是rsLoRA。
3. **实验验证rsLoRA的优势**：为了看看rsLoRA是不是真的好用，作者做了好多实验。选了Llama 2模型和OpenOrca指令调整数据集，用AdamW优化器来训练。在实验里，让$ r $取不同的值（$ r \in \{4,8,32,128,512,2048\} $ ），对比LoRA和rsLoRA的表现。衡量的指标有两个，一个是微调困惑度，简单说就是模型预测结果的“靠谱程度”，困惑度越低说明模型预测越准；另一个是平均参数梯度范数，它能反映训练过程中梯度的变化情况。从微调困惑度的结果来看，LoRA模型不管$ r $怎么变，最后都差不多收敛到同一个损失值，有些高秩的甚至表现更差；但rsLoRA模型随着$ r $增大，微调困惑度明显降低，也就是性能更好。再看平均参数梯度范数，LoRA在$ r $高的时候，梯度范数会急剧下降，就像前面说的发动机出故障，这就是梯度崩溃；而rsLoRA在训练开始时，不同$ r $的梯度范数都差不多，而且在整个训练过程中，梯度范数都能保持在相近的水平，说明它训练很稳定。 除了这个基础实验，作者还做了很多其他实验来进一步验证rsLoRA的优势： 
    - 换了不同的模型（像GPT-J）、数据集（比如GSM8k基准数据集 ）和优化器（例如Adafactor），结果还是rsLoRA表现更好，说明它不是只在特定情况下好用，有通用性。 
    - 用随机梯度下降（SGD）这种不同的训练方法，rsLoRA依然很稳定，排除了是AdamW优化器让它看起来稳定的可能。 
    - 调整LoRA里秩为4时的学习率，发现就算把学习率调来调去，它的性能还是比不上rsLoRA高秩时的性能，说明高秩和rsLoRA这种方法本身对提升学习效果很重要，不是简单调调学习率就能达到同样效果的。
4. **研究总结与展望**：这篇论文通过理论分析和大量实验，找到了LoRA里缩放因子的问题，提出了rsLoRA方法。和原来的LoRA比，rsLoRA在高秩适配器的情况下，训练更稳定，性能提升也很明显。这样一来，以后微调大语言模型的时候，我们就可以根据自己能使用的内存情况，大胆选择更高的秩，让模型达到更好的微调效果。未来还可以研究把rsLoRA用在其他类似的微调方法里，比如AdaLoRA，说不定能让这些方法在高秩预算的情况下，微调性能变得更好。

# <font style="color:rgb(28, 31, 35);">2404-PISSA: PRINCIPAL SINGULAR VALUES AND SINGULAR VECTORS ADAPTATION OF LARGE LANGUAGE MODELS</font>
> [https://arxiv.org/pdf/2404.02948](https://arxiv.org/pdf/2404.02948)
>

+ 在大语言模型（LLMs）的应用场景里，我们常常需要对模型进行微调，让它在特定任务上表现得更好。然而，直接对大规模的语言模型进行全面微调，成本高得离谱。就好比给一个有超多零件的精密机器每个零件都调整一遍，不仅耗费大量时间和计算资源，对硬件的要求也极高，像LLaMA 65B参数模型进行常规16位微调，需要超过780GB的GPU内存，这使得普通设备根本无法承担。 为了解决这个问题，人们想出了很多办法，其中低秩适应（LoRA）是比较受欢迎的一种。LoRA的思路是这样的：假设我们要调整的模型参数矩阵是$ W $（$ W \in \mathbb{R}^{m ×n} $ ），它通过两个小很多的矩阵$ A $（$ A \in \mathbb{R}^{m ×r} $ ）和$ B $（$ B \in \mathbb{R}^{r ×n} $ ）的乘积来近似表示对$ W $的调整量$ \Delta W $ ，也就是$ \Delta W = AB $ ，这里$ r $远远小于$ m $和$ n $ 。在开始训练的时候，$ A $是用随机的高斯噪声来初始化的，$ B $则被设为0 。这样做的好处是，刚开始加入这个“小调整”模块时，不会改变模型原本的输出。
+ 但它也有个大问题，因为$ A $和$ B $一开始的取值情况，导致在训练初期，计算出来的梯度很小。这就好比汽车启动时动力不足，使得模型在微调过程中收敛得很慢，要花很长时间才能调整到比较好的状态。 那有没有更好的办法呢？于是，Principal Singular values and Singular vectors Adaptation（PiSSA）方法就诞生了。
+ PiSSA同样采用了类似LoRA的低秩适配器结构，但在初始化的时候有了新的思考。它先对原来的权重矩阵$ W $做奇异值分解（SVD），把$ W $分解成$ W = USV^{T} $ 。这里的$ U $、$ V $是奇异向量矩阵，它们的列向量是相互正交的；$ S $是一个对角矩阵，对角线上的元素就是奇异值，并且这些奇异值是按照从大到小的顺序排列的（$ s \in \mathbb{R}_{≥0}^{min (m, n)} $ ）。 
    - 根据奇异值的大小，PiSSA把$ W $分成了两部分：一部分是主要低秩矩阵$ W^{pri} $ ，它由那些比较大的奇异值对应的部分组成；另一部分是残差矩阵$ W^{res} $ ，由剩下较小的奇异值对应的部分构成。$ W^{pri} $可以用$ A $和$ B $来表示，具体的计算方式是$ A = U_{[:,: r]} S_{[: r,: r]}^{1 / 2} $ ，$ B = S_{[: r,: r]}^{1 / 2} V_{[:,: r]}^{T} $ 。在微调的时候，$ A $和$ B $是可以训练的，而$ W^{res} $则被冻结起来，不再调整。 
    - 为什么要这么做呢？因为那些大的奇异值对应的部分，就像是模型里最关键、最核心的“零件”，直接调整这些部分，模型就能更快、更精准地适应新的任务，就像直接抓住了问题的关键，能更快地找到解决办法。从数学原理上讲，**主要奇异向量代表了矩阵**$ W $**在哪些方向上的变化对整体影响最大，PiSSA通过调整这些关键方向，能让模型更高效地学习。** 
    - 而且，PiSSA在量化方面也有优势。量化是一种减少内存消耗的技术，简单来说就是把数值的范围进行划分，用更少的位数来表示数值。QLoRA是把整个模型的权重矩阵$ W $进行量化，而PiSSA只对$ W^{res} $进行量化。由于$ W^{res} $去除了大奇异值成分，它的分布范围比$ W $更窄，就像把一个很宽的范围缩小了，这样在进行量化的时候，产生的误差就更小。 研究人员做了大量实验来验证PiSSA的效果。他们在12种不同规模的模型上进行实验，涵盖了自然语言生成（NLG）和自然语言理解（NLU）的多种任务。在自然语言生成的GSM8K基准测试中，用PiSSA微调的Mistral - 7B模型准确率达到72.86% ，而用LoRA微调的只有67.7% ，PiSSA比LoRA高出了5.16% 。在自然语言理解的GLUE基准测试中，PiSSA在大部分实验设置下都比LoRA表现得更好。 总的来说，PiSSA这种方法通过新的初始化思路，解决了LoRA收敛慢的问题，在微调效率和性能上都有很大提升，为大语言模型的参数高效微调提供了更好的选择。

# <font style="color:rgb(31, 35, 41);">2402-DoRA: Weight-Decomposed Low-Rank Adaptation</font>
> [https://arxiv.org/pdf/2402.09353](https://arxiv.org/pdf/2402.09353)
>

在自然语言处理和多模态任务中，预训练模型发挥着重要作用。为了让这些通用模型适配特定的下游任务，通常会采用微调的方式。但随着模型和数据集规模的不断扩大，对整个模型进行全量微调的成本变得极高。因此，参数高效微调（PEFT）方法应运而生，其中LoRA因其简单高效且不增加额外推理成本而受到广泛关注。然而，LoRA与全量微调（FT）之间仍存在一定的精度差距。本文针对这一问题展开研究，提出了Weight-Decomposed Low-Rank Adaptation（DoRA）方法，旨在缩小这一差距。

1. **研究背景与动机**：预训练模型在众多领域表现出色，但全量微调成本高昂。PEFT方法通过训练少量参数来适应下游任务，LoRA是其中的代表方法，但它和FT的精度差异原因尚未深入探究。本研究希望找到差距原因并改进LoRA，提升其性能。
2. **相关工作**：PEFT方法主要分为三类。基于适配器的方法通过在原模型中引入额外的可训练模块；基于提示的方法则是在输入中添加软令牌进行微调；而基于低秩矩阵近似的方法，如LoRA及其变体，通过低秩矩阵近似权重变化，在不改变模型架构和增加推理负担的情况下进行微调。本文聚焦于这一类方法。
3. **LoRA和FT的模式分析**
    - **LoRA**：LoRA假设在微调过程中，权重的更新具有低“内在秩” 。对于一个预训练的权重矩阵$ W_{0} \in \mathbb{R}^{d ×k} $，它用两个低秩矩阵$ B \in \mathbb{R}^{d ×r} $和$ A \in \mathbb{R}^{r ×k} $（$ r \ll min (d, k) $）的乘积$ BA $来表示权重更新$ \Delta W $ 。那么，微调后的权重$ W' $就可以表示为$ W' = W_{0} + \Delta W = W_{0} + BA $。在训练开始时，矩阵$ A $用均匀的Kaiming分布初始化，$ B $初始化为零，这样$ \Delta W $在训练开始时为零。在推理阶段，LoRA可以将学习到的$ \Delta W $与预训练权重$ W_{0} $合并，且不会引入额外的延迟。
    - **权重分解分析**：以往研究认为LoRA和FT的精度差异主要是由于可训练参数数量有限，但未深入分析其他潜在原因。本文受权重归一化的启发，将权重矩阵分解为幅度和方向两个分量，以此揭示LoRA和FT学习模式的内在差异。具体来说，对于权重矩阵$ W \in \mathbb{R}^{d ×k} $，其分解公式为$ W = m \frac{V}{\| V\| _{c}} = \| W\| _{c} \frac{W}{\| W\| _{c}} $，其中$ m \in \mathbb{R}^{1 ×k} $是幅度向量，$ V \in \mathbb{R}^{d ×k} $是方向矩阵，$ \|\cdot\|_{c} $是矩阵按列向量的向量范数。通过对VL-BART模型在四个图像文本任务上的案例研究发现，LoRA在训练过程中，幅度和方向的变化呈现出一致的正斜率趋势，即两者的变化成比例关系。而FT的学习模式则更加多样化，斜率相对为负。这表明LoRA在同时进行幅度和方向的细微调整时存在困难，而FT则更擅长这种精细的调整。
4. **DoRA方法**
    - **权重分解低秩适应**：基于上述分析结果，本文提出了DoRA方法。DoRA首先将预训练权重分解为幅度和方向两个分量，然后分别对它们进行微调。由于方向分量的参数数量较多，DoRA进一步使用LoRA对方向分量进行调整，以实现高效的微调。其公式为$ W' = \underline{m} \frac{V+\Delta V}{\| V+\Delta V\| _{c}} = \underline{m} \frac{W_{0}+\underline{B A}}{\left\| W_{0}+\underline{B A}\right\| _{c}} $，其中$ \Delta V $是通过两个低秩矩阵$ B $和$ A $相乘得到的增量方向更新，带下划线的参数表示可训练参数。在初始化时，DoRA使用预训练权重$ w_{0} $，此时$ m=\left\|W_{0}\right\|_{c} $，$ V = W_{0} $，之后保持$ V $冻结，$ m $作为可训练向量。通过这样的设计，DoRA的学习模式更接近FT，实验也证明了它比LoRA具有更强的学习能力。
    - **DoRA的梯度分析**：对DoRA的梯度进行推导，可以得到关于$ m $和$ V' = V+\Delta V $的梯度公式：$ \nabla_{V'} \mathcal{L}=\frac{m}{\left\| V';\right\| _{c}}\left(I-\frac{V' V^{\prime T}}{\left\| V'\right\| _{c}^{2}}\right) \nabla_{W'} \mathcal{L} $，$ \nabla_{m} \mathcal{L}=\frac{\nabla_{W'} \mathcal{L} \cdot V'}{\left\| V'\right\| _{c}} $ 。从这些公式可以看出，权重梯度$ \nabla_{W'} L $被$ m /\left\|V'\right\|_{c} $缩放，并投影远离当前权重矩阵，这使得梯度的协方差矩阵更接近单位矩阵，有利于优化。同时，这种分解带来的优化优势完全转移到了$ \Delta V $上，增强了LoRA的学习稳定性。进一步分析发现，DoRA在更新时，当方向更新较小时，幅度更新相对较大；反之，方向更新较大时，幅度更新相对较小。这种学习模式与FT更为相似，而与LoRA不同。
    - **降低训练开销**：在DoRA中，由于低秩适应方向的调整，其低秩更新的梯度与$ W' $的梯度不同，这在反向传播时需要额外的内存。为解决这个问题，DoRA将$ \|V+\Delta V\|_{c} $视为常数，从梯度图中分离它。这样，关于$ m $的梯度不变，$ \nabla_{V} L $重新定义为$ \nabla_{V'} \mathcal{L}=\frac{m}{C} \nabla_{W'} \mathcal{L} $（$ C=\left\| V'\right\| _{c} $）。通过这种方式，大大减少了梯度图的内存消耗，且对精度影响极小。实验表明，在微调LLaMA和VL-BART模型时，分别能减少约24.4%和12.4%的训练内存。
5. **实验**
    - **常识推理**：在常识推理任务中，使用LLaMA-7B/13B、LLaMA2-7B和LLaMA3-8B模型，对比DoRA与LoRA以及其他基线方法（如Prompt learning、Series adapter、Parallel adapter）。结果显示，DoRA在所有模型上均超越了基线方法。在LLaMA-7B模型上，DoRA比LoRA的精度提升了3.7%，甚至超过了ChatGPT的精度。即使将DoRA的秩减半（表示为$ DoRA^{\dagger} $），其性能仍优于LoRA。这表明DoRA能够有效提升LoRA的学习能力，减少对高秩的依赖。
    - **图像/视频-文本理解**：在基于VL-BART模型的多个图像/视频-文本理解任务中（如VQA、GQA、NVLR2、COCO Caption、TVQA、How2QA、TVC、YC2C），对比DoRA与LoRA和全量微调。结果表明，DoRA在保持相似可训练参数数量的情况下，在所有任务中均超越了LoRA，在图像文本理解任务中精度比LoRA提高了近1%，在视频文本理解任务中提高了约2%，接近全量微调的精度。
    - **视觉指令调整**：在LLaVA-1.5-7B模型的视觉指令调整任务中，DoRA同样优于LoRA和全量微调。尽管在该任务中全量微调可能存在过拟合问题，导致LoRA的表现超过全量微调，但DoRA仍比LoRA的平均精度提高了0.7%，比全量微调提高了1.1%。
    - **与其他LoRA变体的兼容性**：以VeRA为例，研究DoRA与其他LoRA变体的兼容性。将VeRA用于DoRA的方向更新，得到DVoRA。在LLaMA-7B和LLaMA2-7B模型的指令调整任务中，对比DVoRA、DoRA、VeRA和LoRA。结果显示，DVoRA结合了DoRA和VeRA的优势，在保持较少参数的情况下，得分与LoRA相当甚至超越，且在不同训练样本数量下均优于VeRA和LoRA。
    - **对不同秩设置的鲁棒性**：通过调整秩（$ r $分别取4、8、16、32、64），在LLaMA-7B模型的常识推理任务中评估DoRA和LoRA的性能。结果表明，DoRA在所有秩配置下均优于LoRA，且当秩低于8时，两者的性能差距进一步扩大。这说明DoRA对不同秩设置具有更好的鲁棒性，能够在较少的可训练参数下实现更高的精度。
    - **调整粒度分析**：研究发现，DoRA可以通过选择性地更新特定模块的幅度和方向分量，减少可训练参数的数量，同时保持甚至提高模型的精度。例如，在LLaMA-7B和LLaMA-13B模型上，通过仅更新多头注意力层的方向和幅度分量以及MLP层的幅度分量，DoRA分别比LoRA的精度提高了2.8%和0.8%，且使用的可训练参数不到LoRA的一半。
6. **更广泛的影响**
    - **QDoRA：对QLoRA的增强**：QLoRA通过将预训练模型量化为4位并在其上微调LoRA来减少训练内存开销。将DoRA应用于QLoRA框架中，形成QDoRA。在Orca-Math数据集上对LLaMA2-7B/LLaMA3-8B模型进行微调实验，结果显示QDoRA不仅在精度上显著超越QLoRA，还略微超过全量微调，同时使用的内存更少。这表明QDoRA有效地结合了QLoRA的参数高效性和全量微调的精细优化能力。
    - **文本到图像生成**：在文本到图像生成任务中，基于SDXL模型，使用DoRA和LoRA按照DreamBooth的训练流程进行微调，使用两个具有挑战性的数据集（3D icons和Lego sets）进行实验。结果表明，在相同的训练设置下，DoRA比LoRA具有更好的个性化效果，能够更准确地反映训练目标。例如，在生成的图像中，DoRA生成的图像能更好地体现训练数据中的独特特征，而LoRA生成的图像则缺少这些特征。
7. **结论**：本文通过一种新颖的权重分解分析，揭示了LoRA和FT在学习模式上的显著差异。在此基础上，提出了DoRA方法，它与LoRA及其变体兼容，学习行为更接近FT。实验结果表明，DoRA在多种微调任务和模型架构上均优于LoRA，且不增加额外的推理开销。未来研究可以探索DoRA在语言和视觉领域之外的应用，如音频领域。

# 2403-<font style="color:rgb(31, 35, 41);">Parameter Efficient Reinforcement Learning from Human Feedback</font>
> [https://arxiv.org/pdf/2403.10704](https://arxiv.org/pdf/2403.10704)
>

在人工智能领域，大型语言和视觉语言模型发展迅速，但让这些模型符合人类的期望和需求至关重要。“Parameter Efficient Reinforcement Learning from Human Feedback”这篇论文聚焦于此，提出了参数高效的人类反馈强化学习（PE - RLHF）方法，下面用更易懂的方式进行解读。

1. **研究背景**：像GPT - 4和Gemini这样的大型模型，虽然在很多任务上表现出色，但它们生成的内容不一定符合人类的偏好。例如，在回答问题时可能提供错误或有害的信息，总结文本时可能抓不住重点。为了解决这个问题，RLHF方法应运而生。RLHF通过训练奖励模型，根据人类的反馈给模型的输出打分，再用这个分数去调整模型的参数，让模型的输出更符合人类的期望。然而，RLHF存在计算成本高和内存占用大的问题，这就限制了它的广泛应用。所以，研究人员思考能否找到一种更高效的方法，在降低计算成本和内存需求的同时，达到与RLHF相似的效果，于是就有了对PE - RLHF的研究。
2. **PE - RLHF方法**
    - **奖励模型训练**：研究人员引入了LoRA（低秩适应）技术来优化奖励模型的训练。想象一下，语言模型就像一个复杂的机器，LoRA适配器就像是给这个机器添加的一个小巧且灵活的“插件”。在训练奖励模型时，只需要训练这个“插件”，而让语言模型的主体部分保持不变（也就是冻结语言模型骨干），这样大大减少了需要训练的参数数量。在训练奖励模型时，从偏好对中学习奖励，其损失函数为$ \mathcal{L}_{r}(\phi)=-\underset{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}{\mathbb{E}}\left[\log \sigma\left(r_{\phi}\left(x, y_{w}\right)-r_{\phi}\left(x, y_{l}\right)\right)\right] $。这里，$ \sigma $是sigmoid函数，$ r_{\phi}(x, y) $表示奖励模型对输入$ x $和输出$ y $给出的奖励分数，$ y_{w} $是人类偏好的响应，$ y_{l} $是非偏好的响应。这个公式的意义在于，通过最小化这个损失函数，让奖励模型学会根据人类的偏好对不同的响应进行准确打分。
    - **策略的强化学习**：在强化学习环节，策略和价值模型同样使用LoRA适配器。这就好比给策略和价值模型这两个“工具”也都装上了灵活的“插件”，只训练“插件”部分，模型主体保持稳定。策略的优化依赖于根据价值模型计算得到的策略梯度。同时，价值模型的训练除了依据奖励分数，还引入了KL散度正则化（$ D_{KL} $），目的是让策略不要偏离初始的锚策略太远，防止出现模型为了获得高奖励而生成低质量或无意义内容的“奖励作弊”现象。策略优化的目标函数为$ J(\theta)=\underset{y \sim \pi_{\theta}(\cdot | x)}{\mathbb{E}}\left[(1-\beta) r_{\phi}(y | x)-\beta D_{KL}\left(\pi_{\theta}^{RL}(y | x) \| \pi^{Anchor }(y | x)\right)\right] $ 。其中，$ \beta $是一个在0到1之间的超参数，用来平衡奖励和KL散度的影响。这个公式表明，策略的优化既要考虑奖励模型给出的奖励，也要考虑与初始锚策略的差异，以确保策略的稳定性和有效性。
3. **数据集和任务**：为了全面评估PE - RLHF的效果，研究人员选取了6个不同的数据集，涵盖5种不同类型的任务。例如在文本摘要任务中，使用Reddit TL;DR和BOLT Message Summarization数据集，检验模型能否把长文本或聊天记录浓缩成简洁、准确的总结；在无害响应生成任务里，利用Anthropic - HH的“Harmlessness”数据集，判断模型生成的回答是否安全、无危害；还有有用响应生成、UI自动化和视觉问答任务，每个任务都有对应的数据集，从不同方面考察模型在各种场景下的表现。
4. **实验设置和指标**：实验选用了PaLM 2和Gemini Pro两种模型。在奖励建模实验中，研究人员调整模型大小和LoRA秩等参数，训练奖励模型，并通过准确率等指标评估其性能。在强化学习实验里，固定奖励模型，训练策略模型，用胜率、准确率等指标来衡量策略模型的效果。例如在文本摘要任务中，使用PaLM 2 L作为“裁判模型”，让它从两个总结（一个是基线总结，一个是策略模型生成的总结）中选择更好的那个，统计策略模型生成的总结的“胜率”，以此来判断策略模型在文本摘要任务上的性能表现。
5. **实验结果**
    - **性能表现**：实验发现，PE - RLHF在奖励建模和强化学习方面的表现与标准RLHF相当。在不同任务中，PE - RLHF奖励模型的准确率和标准RLHF奖励模型差不多，而且它训练的参数极少，不到大模型总参数的0.1%；在强化学习策略训练上，PE - RLHF训练出的策略和标准RLHF训练的策略效果相近，在文本相关任务中训练的参数不到总参数的0.1%，在涉及视觉和文本的任务中不到0.2%。
    - **模型大小和LoRA秩的影响**：改变LoRA秩对奖励模型性能的影响较小，但随着模型骨干变大，PE - RLHF在奖励建模上更接近标准全量微调的效果；在强化学习中，LoRA秩越高，PE - RLHF策略的性能越好，并且模型骨干越大，效果也越好。
    - **内存和速度优势**：PE - RLHF在内存使用和训练速度上展现出明显的优势。由于训练的参数大幅减少，在奖励模型训练时，它只需要标准训练所需峰值HBM（高带宽内存）的43 - 74%，强化学习时只需要74 - 80%。训练速度也显著提升，奖励模型训练速度是标准训练的1.15 - 1.9倍，强化学习训练速度是标准训练的1.15 - 1.24倍。
6. **研究结论**：PE - RLHF是一种很有潜力的方法，它在减少内存使用和训练时间的同时，能够达到与标准RLHF相似的性能。不过，它也存在一些需要改进的地方，比如可能会出现过拟合的问题，在数据使用效率方面还需要进一步探索，而且研究只针对LoRA这一种参数高效微调方法，未来可以对比研究其他类似方法。后续研究可以从更广泛的泛化、防止奖励作弊以及开源代码等方向展开，进一步提升PE - RLHF的性能和应用范围。

# 2409-UNLEASHING THE POWER OF TASK-SPECIFIC DIRECTIONS IN PARAMETER EFFICIENT FINE-TUNING
> [https://arxiv.org/pdf/2409.01035v1](https://arxiv.org/pdf/2409.01035v1)
>

在自然语言处理领域，大语言模型（LLMs）展现出强大能力，但对其进行全量微调时，由于模型参数规模庞大（从数亿到数千亿不等），需要消耗大量的计算资源和内存，这严重限制了LLMs在不同场景下的实际应用。为解决该问题，参数高效微调（PEFT）技术应运而生，其中LoRA（低秩适应）方法备受关注。本文围绕LoRA中任务特定方向（TSD）展开研究，提出了新的框架和方法，旨在更有效地利用这些方向提升模型性能。

### LoRA方法及存在的问题
1. **LoRA原理**：LoRA基于权重更新通常具有低内在秩的发现，对于每一层的权重矩阵$ W \in \mathbb{R}^{n ×m} $，将其变化$ \Delta W $建模为两个低秩矩阵$ A \in \mathbb{R}^{n ×r} $和$ B \in \mathbb{R}^{r ×m} $的乘积（$ r \ll{n, m} $），即$ \Delta W = AB $。在训练时，仅对$ A $和$ B $进行更新，$ W $保持不变；推理时，将低秩矩阵融入$ W $，不会增加额外的计算或内存成本 。例如，对于原始输出$ h = Wx $，经过LoRA调整后，前向传递变为$ h = (W + AB)x $。
2. **LoRA中TSD的矛盾**：LoRA引入了TSD概念，认为它是$ W $在预训练时已学习但未强调的方向，通过$ \Delta W $进行放大可提升特定下游任务的性能。然而，LoRA对TSD的定义和使用存在问题。它指出TSD是$ \Delta W $的顶部奇异向量方向，但这与TSD应独立于学习到的$ \Delta W $这一特性相矛盾。从LoRA的结论还能推断出$ \Delta W $的顶部奇异方向在$ W $的方向之中，但实际中两者很难完全对齐。这些矛盾表明LoRA对TSD的定义不清晰，影响了其性能发挥和对微调机制的深入理解。

### 关于任务特定方向的新框架
1. **相关定义和概念**：基于矩阵的奇异值分解（SVD），定义了矩阵的核心基和全局基。对于矩阵$ A = U \sum V^{\top} $（$ n < m $），核心基为$ {u_{i} v_{i}^{\top}} $，全局基为$ {u_{i} v_{j}^{\top}} $（所有$ i, j $的组合）。同时，定义矩阵$ A $的方向为其扩展的奇异值（用零填充）组成的向量，即$ (\sigma_{1}, 0, \cdots, 0, \sigma_{2}, 0, \cdots, 0, \cdots, \sigma_{n}, \cdots, 0) \in \mathbb{R}^{n m} $ 。
2. **TSD的精确定义**：对于特定任务，在矩阵空间$ \mathbb{R}^{n ×m} $中存在一个最优矩阵$ W^{*} $。预训练矩阵$ W $针对该任务的最优改变为$ \Delta W^{*} = W^{*} - W $。通过将$ \Delta W^{*} $和$ W^{*} $投影到$ W $的全局基上，来捕捉它们的方向。具体定义投影算子$ \Pi_{W}(A) $，其中$ \Pi_{W}(A) = (p_{11}, \cdots, p_{n m}) \in \mathbb{R}^{n m} $，$ p_{i j} = u_{i}^{\top} A v_{j} $（$ u_{i} $和$ v_{j} $是$ W $的左右奇异向量）。在此基础上，计算$ W $的核心方向坐标值的变化率$ \delta_{i} $，公式为$ \delta_{i}=\left|\frac{\Pi_{W}\left(W^{*}\right)_{i i}-\sigma_{i}}{\sigma_{i}+\epsilon}\right|=\left|\frac{u_{i}^{\top} W^{*} v_{i}-u_{i}^{\top} W v_{i}}{\sigma_{i}+\epsilon}\right|=\left|\frac{u_{i}^{\top} \Delta W^{*} v_{i}}{\sigma_{i}+\epsilon}\right|=\left|\frac{\Pi_{W}\left(\Delta W^{*}\right)_{i i}}{\sigma_{i}+\epsilon}\right| $（$ \epsilon = 10^{-6} $是为防止奇异值为零的常数）。最终将TSD定义为$ W $的核心方向中，坐标值在从$ W $到$ W^{*} $的变化过程中，变化率$ \delta $显著较高的那些方向。
3. **TSD的性质**：通过在常识推理任务上对LLaMA - 7B进行全量微调实验发现，TSD主要对应$ W $中与较小奇异值相关的核心方向，但并非是最小奇异值对应的方向；而且TSD只包含少数变化率较大的方向，大多数其他核心方向变化率极小。
4. **使用TSD面临的挑战及解决思路**：在实际微调场景中，由于在微调前$ \Delta W^{*} $和$ W^{*} $都是未知的，所以很难直接利用TSD信息。不过研究发现，尽管如此，LoRA训练过程中得到的$ \Delta W $始终能够捕获TSD的信息。这一结论为后续利用TSD提供了关键的基础。

### LoRA - Dash方法
1. **预启动阶段**：在这个阶段，LoRA - Dash首先训练矩阵$ A $和$ B $（即LoRA的$ \Delta W $）来捕获TSD信息。基于前面提到的$ \Delta W $能捕获TSD信息的结论，设定一个预定义的训练步数$ t $（文中建议设为100），认为经过$ t $步训练后，$ \Delta W $可以持续捕获TSD信息。然后，将$ \Delta W $投影到$ W $的核心方向上计算变化率，公式为$ \delta_{i}=\frac{u_{i}^{\top} \Delta W v_{i}}{\sigma_{i}+\epsilon} $。选取变化率最高的前$ s $个核心方向（文中建议$ s = 8 $），将其作为“启动TSD”（LTSD） 。
2. **冲刺阶段**：为了充分利用识别出的LTSD，LoRA - Dash直接学习这些方向坐标的变化，用$ \Delta \sigma_{i} $表示第$ i $个$ \overline{u}_{i} \overline{v}_{i}^{\top} $的坐标变化（初始化为零）。最终的更新权重矩阵为$ W+\Delta W_{all }=W+\Delta W_{A B}+\Delta W_{d a s h}=W+A B+\sum_{i=1}^{s} \Delta \sigma_{i} \overline{u}_{i} \overline{v}_{i}^{\top} $。在训练过程中，$ A $、$ B $和$ \Delta \sigma_{i} $会持续更新。

### LoRA - Dash的性能验证
1. **数值结果**：在常识推理和自然语言理解任务中进行实验，结果表明LoRA - Dash在所有任务和模型上的表现都显著优于LoRA。例如，在对LLaMA - 7B进行微调的实验中，LoRA - Dash在较低的参数预算下，依然能够保持较高的性能，这显示出它对参数预算的鲁棒性；而且在某些情况下，LoRA - Dash的性能甚至超过了全量微调的结果。
2. **视觉结果**：在主题驱动生成任务中，通过对比LoRA和LoRA - Dash生成的图像发现，LoRA - Dash生成的图像与输入图像的主题更加契合，能够更好地遵循给定的提示，生成的图像在细节和语义表达上更加准确和丰富。

### 对LoRA - Dash的深入理解
1. **与TSD的关系**：通过实验考察LoRA - Dash识别的LTSD、最终训练后的方向（DTSD）与真实TSD之间的对齐情况。结果发现，LoRA - Dash能够捕获大量的TSD信息，LTSD和DTSD紧密对齐，且LoRA - Dash训练后的权重比普通LoRA能捕获更大比例的TSD信息。
2. **对TSD的放大效果**：定义了一个放大因子来量化LoRA - Dash对TSD信息的放大程度，公式为$ \frac{\left\| \overline{U}^{\top}\left(W+\Delta W_{all }\right) \overline{V}\right\| _{F}}{\left\| \overline{U}^{\top} W \overline{V}\right\| _{F}} $（同时也分别评估了$ \Delta W_{A B} $和$ \Delta W_{dash} $的放大因子）。实验结果表明，LoRA - Dash在所有测试条件下都能显著增强与LTSD相关的特征，其中$ \Delta W_{dash} $对整体特征增强的贡献占主要部分，这使得$ \Delta W_{A B} $在微调时可以更专注于学习其他重要特征。
3. **与其他核心方向的比较**：研究发现，在预启动阶段选择TSD进行微调的性能最佳，选择底部核心方向次之，选择顶部核心方向最差。这是因为TSD代表了模型权重在适应新任务时变化最大的方向，直接利用TSD能最大程度地提升模型对下游任务的适应性。
4. **超参数的影响**：LoRA - Dash中的超参数$ t $和$ s $对性能有影响。实验表明，预启动阶段过长可能导致性能下降，因为较早进入冲刺阶段可以更充分地利用TSD；而冲刺阶段使用的方向数量$ s $过大时，可能会引入一些无关方向，导致训练过程中的噪声增加，从而影响模型性能。
5. **对其他方法的提升作用**：将TSD应用到其他PEFT方法（如AdaLoRA和FLoRA）中，结果显示这些方法的性能也得到了显著提升，这进一步证明了TSD在优化模型行为方面的重要性和通用性。

### 研究结论
本文重新审视了LoRA对TSD的探索，指出其在理解TSD方面的不足。通过提出精确的TSD定义和性质研究框架，引入LoRA - Dash方法，充分释放了TSD的潜力。大量实验和深入分析验证了LoRA - Dash相较于传统方法的显著优势，突出了TSD在实现卓越任务特定性能中的关键作用。该研究为参数高效微调技术的发展提供了新的思路和方法，有望推动自然语言处理及相关领域的进一步发展。

# <font style="color:rgb(31, 35, 41);">2406-MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning</font>
> [https://arxiv.org/pdf/2406.09044](https://arxiv.org/pdf/2406.09044)
>

MiLoRA方法，通过仅更新权重矩阵的次要奇异分量来微调大语言模型（LLM），有效提升了模型性能，且不牺牲训练和推理效率。

1. **研究背景与动机**
    - **LLM微调困境**：大语言模型在很多任务上表现出色，它先在大规模数据上预训练，然后通过微调适应不同下游任务。但完全微调所有参数需要大量计算资源，这限制了LLM在不同场景的应用。
    - **现有方法问题**：参数高效微调方法（PEFT）旨在降低计算成本，其中LoRA应用广泛。LoRA通过更新两个低秩矩阵来调整模型权重，同时冻结预训练权重。不过，它随机初始化低秩矩阵，这样可能会覆盖预训练权重中重要的特征，影响模型性能。
    - **研究目的**：本文提出MiLoRA方法，希望在微调时更好地利用预训练知识，减少对预训练特征的干扰，提高微调效果。
2. **方法原理**
    - **奇异值分解（SVD）**：对于矩阵$ W \in \mathbb{R}^{m×n} $ ，其奇异值分解公式为$ W = U\sum V^{\top} $ 。其中，$ U \in \mathbb{R}^{m×m} $ 、$ V \in \mathbb{R}^{n×n} $ 分别是由左右奇异向量组成的矩阵，$ \sum \in \mathbb{R}^{m×n} $ 是对角矩阵，对角线上的元素是$ W $的奇异值，且按从大到小排列。假设$ m \leq n $ ，$ W $还可以写成$ W = \sum_{i = 1}^{m} \sigma_{i} u_{i} v_{i}^{\top} $ ，这里的$ \sigma_{i} $ 就是奇异值，$ u_{i} $ 和$ v_{i} $ 分别是$ U $和$ V $的第$ i $列向量。
    - **MiLoRA分解策略**：基于SVD，MiLoRA把权重矩阵$ W $分成主要矩阵$ W_{p} $和次要矩阵$ W_{m} $ ，即$ W = W_{p} + W_{m} $ 。其中，$ W_{p} = \sum_{i = 1}^{m - r} \sigma_{i} u_{i} v_{i}^{\top} $ ，对应较大的奇异值，包含预训练知识的关键部分；$ W_{m} = \sum_{i = m - r + 1}^{m} \sigma_{i} u_{i} v_{i}^{\top} $ ，对应较小的奇异值，包含噪声或长尾信息。在微调过程中，MiLoRA固定$ W_{p} $ ，用$ W_{m} $初始化低秩矩阵$ A_{m} $和$ B_{m} $ ，也就是$ W_{m} = (U_{m} \sqrt{\sum_{m}})(\sqrt{\sum_{m}} V_{m}^{\top}) = B_{m} A_{m} $ 。这样能让模型在微调时利用次要奇异向量所张成的空间学习，减少对预训练知识的干扰，且无需调整除秩$ r $以外的超参数。
3. **实验设置**
    - **实验任务与数据集**：为评估MiLoRA，在多种任务和数据集上进行实验。常识推理任务使用了如BoolQ、PIQA等8个数据集；数学推理任务使用GSM8K和MATH测试集；指令跟随任务用Alpaca-Eval v1.0评估；视觉指令跟随任务在7个视觉语言任务基准上评估。
    - **对比方法**：将MiLoRA与LoRA、PiSSA等方法对比，部分实验还对比了rsLoRA、LoRA+等其他方法。
    - **实验环境与参数设置**：大部分实验在8块NVIDIA L40 GPU上进行，超参数设置参照LLM-Adapters或相关论文，不同任务设置不同的秩、学习率等超参数。
4. **实验结果与分析**
    - **主要任务表现**：在多个任务和数据集上，MiLoRA的表现都超过了LoRA和PiSSA。例如在常识推理任务中，基于LLaMA2 - 7B模型，MiLoRA的平均准确率比LoRA高1.6；在数学推理任务中，基于LLaMA2 - 7B模型，MiLoRA的平均EM得分比LoRA高2.0。
    - **与更多方法对比**：在数学和代码任务实验中，MiLoRA同样优于rsLoRA、LoRA+等方法，而且它的训练效率和LoRA差不多。
    - **关键因素探究**：实验发现，用次要奇异分量初始化低秩矩阵时，模型性能最佳；通过分析$ \Delta W $（微调后与预训练权重的差异）和$ W $的关系可知，MiLoRA对$ W $中已有的特征影响较小；MiLoRA的遗忘损失最低，说明它对预训练知识的修改最少，保留得更好。
    - **与PiSSA对比**：PiSSA也是一种低秩适应方法，它通过调整主要奇异分量来近似全量微调，而MiLoRA是调整次要奇异分量以保留模型知识。在合适的超参数设置下，MiLoRA的性能明显优于PiSSA。
5. **研究结论**：MiLoRA是一种简单有效的LLM低秩适应方法，通过调整次要奇异分量，在不降低训练和推理效率的情况下提升了微调性能。不过，目前研究主要集中在LLaMA系列模型和部分任务上，未来可拓展到其他模型和任务进行研究。

# <font style="color:rgb(31, 35, 41);">2401-RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation</font>
> [https://arxiv.org/pdf/2401.04679](https://arxiv.org/pdf/2401.04679)
>

“RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation”这篇论文主要提出了一种名为RoSA（Robust Adaptation）的新方法，用于大语言模型（LLMs）的参数高效微调，在提升模型性能的同时降低计算和内存成本。下面我将从研究背景、方法、实验和结论等方面详细总结。

1. **研究背景**
    - **LLMs的现状和问题**：LLMs发展迅速，但训练成本极高，尤其是从头开始训练。因此，利用有限数据对LLMs进行微调成为常用方法。然而，全量微调（FFT）所有参数的内存成本过高，难以在标准GPU上执行。
    - **PEFT方法的出现**：为解决FFT的问题，参数高效微调（PEFT）方法应运而生。这些方法只优化部分参数，降低了计算和内存成本。其中，LoRA方法通过训练低秩“适配器”层，在一定程度上恢复了模型精度，还具有隐式正则化和简化超参数搜索的优势。
    - **现有方法的不足**：LoRA在复杂任务上难以达到FFT的精度。这是因为复杂任务的微调更新结构复杂，低秩假设无法有效捕捉这些更新，会过滤掉重要信息。所以，寻找一种既具备LoRA的实用性和易用性，又能达到FFT高精度的PEFT方法成为研究的关键。
2. **相关工作**
    - **参数高效微调方法**：除了LoRA，还有其他PEFT方法，如FISH Mask和DSEE。FISH Mask基于Fisher信息矩阵生成稀疏掩码，但在LLMs中不实用；DSEE结合低秩和稀疏适配器，但在生成掩码和系统支持方面存在问题。
    - **稀疏训练和微调**：稀疏化是降低模型计算和内存需求的常用策略。稀疏微调在固定稀疏掩码下对预训练模型进行微调，但与稀疏适应不同，稀疏适应只是冻结未参与的权重，能实现更高的稀疏度。
    - **稳健主成分分析（RPCA）**：RPCA旨在处理包含异常值的数据，它将数据分解为低秩和稀疏矩阵的和。这为LLMs微调提供了思路，即通过类似的分解更好地逼近微调更新。
    - **系统支持**：在系统层面，对稀疏表示的支持面临挑战，尤其是在GPU上。Sputnik库提供了一定的加速，但RoSA通过改进索引方案和引入新的内核，在非结构化稀疏情况下取得了更好的性能提升。
3. **大语言模型的适应方法**
    - **符号表示与现有方法**：定义了LLM的参数、权重层等符号，如用$ N $表示预训练的LLM，$ W $表示其权重层序列。FFT通过求解$ min _{\mathcal{W}, \overline{w}} \mathcal{L}(\mathcal{D} ; \mathcal{W}, \overline{w}) $来优化所有参数；LoRA则通过约束扰动矩阵的秩，如$ min _{\Delta} \mathcal{L}(\mathcal{D} ; \mathcal{W}+\Delta, \overline{w}), s.t. \forall 1 \leq i \leq k: rank\left(\Delta_{i}\right) \leq r $ ，减少可训练权重数量；SpA通过约束扰动的稀疏性，如$ min _{\Delta} \mathcal{L}(\mathcal{D} ; \mathcal{W}+\Delta, \overline{w}), s.t. \forall 1 \leq i \leq k:\left\| \Delta_{i}\right\| _{0} \leq d m_{i} n_{i} $ ，降低参数数量。
    - **RoSA方法**：受RPCA启发，RoSA认为低秩加稀疏矩阵能更好地逼近FFT的更新。其优化目标为$ min _{\Delta^{L}, \Delta^{S}} \mathcal{L}\left(\mathcal{D} ; \mathcal{W}+\Delta^{L}+\Delta^{S}, \overline{w}\right), s.t. \forall 1 \leq i \leq k:\left\{\begin{array}{c} rank\left(\Delta_{i}^{L}\right) \leq r \\ \left\| \Delta_{i}^{S}\right\| _{0} \leq d m_{i} n_{i} \end{array}\right. $ ，即联合训练低秩适配器$ \Delta^{L} $和稀疏适配器$ \Delta^{S} $。通过特定算法生成稀疏掩码，然后共同优化两个适配器。
4. **系统实现**
    - **存储格式**：低秩适配器存储为两个矩阵$ B $和$ A $的乘积，$ B $维度为$ m × r $，$ A $维度为$ r × n $；稀疏适配器采用压缩稀疏行（CSR）格式存储，利用三个列表表示稀疏矩阵。
    - **前向和反向传播**：前向传播时，计算层输出$ O = X\left(W+\Delta^{L}+\Delta^{S}\right) $ ，通过优化计算过程提高效率；反向传播计算参数和输入梯度，针对SDDMM操作进行优化，利用掩码结构减少计算量。
    - **梯度累积**：在生成掩码时，将每个权重矩阵的梯度计算后及时转移到CPU，减少GPU内存占用，且不显著影响运行时间。
5. **实验**
    - **实验设置**：将RoSA集成到PEFT库，在LLaMA2 - 7B模型上对ViGGO、GSM8k和SQL生成三个数据集进行微调实验。设置了一系列超参数，如批量大小、优化器参数、学习率调度器等，并对不同方法进行了仔细调优。
    - **实验结果**：在单epoch和多epoch训练中，RoSA在大多数任务和参数预算下优于LoRA和SpA。在一些任务上，RoSA甚至超过了FFT的精度。例如，在GSM8k数据集上，RoSA在某些参数设置下的准确率比LoRA高很多。QRoSA在量化预训练权重后，在GSM8k数据集上也取得了不错的效果。此外，实验还表明RoSA的掩码选择对性能影响很大，按任务选择掩码能获得更好的性能。不过，当前RoSA的实现比LoRA慢1.7 - 2倍，但考虑到其精度优势，在实际应用中是可接受的。
6. **讨论**
    - **研究结论**：RoSA在相同参数预算下显著优于LoRA和其他方法，在许多设置下能匹配甚至超过FFT的精度。论文还提供了RoSA在PyTorch中的高效实现，使其成为研究人员可用的工具。
    - **未来展望**：RoSA为LLMs的高效微调提供了新途径，但仍有改进空间，如进一步优化运行时间。在资源受限的场景下，RoSA是一种非常有潜力的微调技术。

# 2405-Spectral Adapter: Fine-Tuning in Spectral Space
> [https://arxiv.org/pdf/2405.13952](https://arxiv.org/pdf/2405.13952)
>

研究提出谱适配器，将预训练权重矩阵谱信息融入微调，在理论和实验上均展现优势，为模型微调提供新方向。

1. **研究背景**
    - **模型微调面临的挑战**：如今语言和视觉模型参数数量暴增，像GPT系列模型。对这些大规模预训练模型进行微调时，全量微调需要巨大的计算资源，存储和交换微调后的模型也很昂贵。比如，一个数十亿参数的模型，全量微调可能需要大量的GPU计算资源和存储空间。
    - **现有解决方案及问题**：为解决这些问题，参数高效微调（PEFT）方法应运而生，其中LoRA模型很成功，它通过添加低秩可训练矩阵来微调，且推理延迟为零。但现有PEFT方法大多只关注减少训练参数，却很少利用预训练模型权重除幅值以外的信息。
    - **为什么从谱信息思考**：之前有研究发现，模型权重的经验谱分布（ESD）有一定的结构信息，能反映模型不同训练阶段，而且模型权重底层谱空间在注意力汇聚现象中起着关键作用。这表明谱信息可能对模型微调有帮助，所以本文想探究能否将谱信息融入微调，提升微调效果。
2. **谱适配器（Spectral Adapter）**
    - **设计思路**：受到LoRA的启发，同时考虑到矩阵的秩和它的谱表示之间的紧密联系，研究人员想到通过对预训练模型的权重矩阵进行奇异值分解（SVD）来微调。公式$ W = USV^{T} $就是权重矩阵的SVD形式，$ U $和$ V $是奇异向量矩阵，$ S $是奇异值矩阵。
    - **两种变体**：加法谱适配器（Spectral AdapterA）是在奇异向量矩阵的顶部列上进行加法调整，公式为$ Spectral Adapter ^{A}(W):=\left[U_{1}+A_{U} U_{2}\right] S\left[V_{1}+A_{V} V_{2}\right] $，$ U_{1} $、$ V_{1} $是$ U $、$ V $的前$ r $列，$ U_{2} $、$ V_{2} $是剩下的列，$ A_{U} $、$ A_{V} $是可训练矩阵。旋转谱适配器（Spectral AdapterR）则是对顶部列进行正交旋转调整，公式为$ Spectral Adapter ^{R}(W):=\left[U_{1} R_{U} U_{2}\right] S\left[V_{1} R_{V} V_{2}\right] $，$ R_{U} $、$ R_{V} $是$ r×r $的可训练正交矩阵。
    - **与以往方法的差异**：以往也有调整模型权重谱表示的方法，但大多是针对特定的视觉模型，而且可训练参数固定，训练时需要存储所有的$ U $、$ S $和$ V $，存储需求大。本文的方法适用于多种类型的权重矩阵，还能灵活选择参数预算，只需要额外存储顶部的奇异向量矩阵部分就行。
3. **理论分析**
    - **适配器秩容量分析**：为了衡量不同适配器调整权重矩阵的能力，研究人员定义了适配器秩容量$ \mathcal{R}\left(f_{\theta} ; W\right):=max _{\theta} rank\left(f_{\theta}(W)\right)-min _{\theta} rank\left(f_{\theta}(W)\right) $。通过证明得出，在相同数量可训练参数的情况下，Spectral AdapterA的秩容量是LoRA适配器的两倍。假设$ W \in \mathbb{R}^{n ×m} $是满行秩矩阵，$ n ≤m $，对于秩为$ r $的LoRA和秩为$ r $的加法谱适配器，有$ \mathcal{R}(LoRA ; W)=r $，$ \mathcal{R}\left( Spectral Adapter ^{A} ; W\right)=2 r $。这意味着Spectral AdapterA调整后的权重有更多的适应自由度，更有利于模型的微调。
    - **权重子空间对齐分析**：以一个简单的两层ReLU网络为例，当数据点分布在低维流形上时，通过推导发现最优神经元应位于数据所在的低维空间。但由于优化误差，部分训练后的神经元可能会偏离。而预训练权重的顶部奇异向量方向能更好地识别这个低维空间的方向，所以微调顶部奇异向量更稳定、更鲁棒。
4. **实验结果**
    - **语言模型微调**：在对DeBERTaV3-base模型和Mistral 7B模型的微调实验中，Spectral AdapterA在GLUE和GSM8K任务上，比LoRA、DoRA等多种PEFT方法表现更出色，获得了更高的分数，且使用的可训练参数更少。
    - **扩散模型融合**：在扩散模型的多适配器融合任务中，传统方法（如简单相加不同的LoRA适配器）会出现身份丢失和概念绑定等问题。Spectral AdapterA通过将不同概念的调整分布在奇异向量矩阵的不同列上，近似于正交基调优，有效解决了这些问题。在多对象生成和多字符生成实验中，生成的图像能更好地识别自定义概念，质量更高。
    - **扩散模型表达能力**：Spectral AdapterR在参数效率方面表现优异。相比LoRA等方法，它只需要更少的训练参数就能达到更好的效果。在对Chilloutmix扩散模型的微调实验中，Spectral AdapterR在低参数预算下就能生成高质量的图像，而且通过Cayley参数化能保持SVD结构，方便后续的微调。
    - **SVD成本分析**：有人可能担心SVD计算会增加训练成本，实验表明，Spectral AdapterA在微调扩散模型和Mistral 7B模型时，运行时和存储开销都很小，现代的数值计算工具还能进一步降低SVD的时间成本。
5. **研究总结与展望**
    - **研究成果**：本文首次提出将谱信息融入微调过程，通过理论分析和大量实验，证明了谱适配器在提升微调效果、参数效率以及解决多适配器融合问题上的优势。
    - **未来研究方向**：后续可以研究对大型模型中不同组件（如注意力层）的谱表示进行微调，还可以探索将谱适应与其他PEFT方法动态结合。另外，需要进一步研究不同奇异向量列的调整对模型的影响，以及开发更快的SVD算法来降低计算成本。

